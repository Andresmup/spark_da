{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa9a896-18d7-492c-8abd-8b4910aca20e",
   "metadata": {},
   "source": [
    "# STRUCT TYPE VS MAP FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd21437-737c-4ec6-88c3-603dfc2f407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5838f59-4df0-4f48-afff-de9848eda5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/30 00:47:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d937a7-351c-4942-86c8-ccec2aedc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, MapType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17317b75-9d23-455d-9ec5-75698b82baf8",
   "metadata": {},
   "source": [
    "## Define Struct Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf860ee-7e9c-4c39-9eee-674bea1c7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_students = StructType([\n",
    "    StructField(\"id\",IntegerType(),False),\n",
    "    StructField(\"names\",StructType([\n",
    "        StructField(\"first_name\",StringType(),True),\n",
    "        StructField(\"middle_name\",StringType(),True),\n",
    "        StructField(\"last_name\",StringType(),True)\n",
    "    ])),\n",
    "    StructField(\"active\",BooleanType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a6a63-095a-4804-8b81-b243479e70fe",
   "metadata": {},
   "source": [
    "## Struct Type Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "896c3d66-1075-4d0e-bd5a-869929892596",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_students_v1 = [\n",
    "    (1, ('Jacob', 'Kyle', 'Smith'), True),\n",
    "    (2, ('Linda', 'Mia', 'Jonh'), False),\n",
    "    (3, ('Oliver', 'James', 'Johnson'), False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f8ac47-d614-4568-831f-97a730d500b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+------+\n",
      "|id |names                   |active|\n",
      "+---+------------------------+------+\n",
      "|1  |{Jacob, Kyle, Smith}    |true  |\n",
      "|2  |{Linda, Mia, Jonh}      |false |\n",
      "|3  |{Oliver, James, Johnson}|false |\n",
      "+---+------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_students_v1 = spark.createDataFrame(data=data_students_v1,schema=schema_students)\n",
    "df_students_v1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11debd7-0c49-4674-90c1-63038dfeedc9",
   "metadata": {},
   "source": [
    "## Struct Type w/Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe3d76f-1ec7-43ac-8d2d-bfaec2c52936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+\n",
      "|id |names               |active|\n",
      "+---+--------------------+------+\n",
      "|1  |{Jacob, NULL, Smith}|true  |\n",
      "|2  |{Linda, Mia, Jonh}  |false |\n",
      "|3  |{NULL, NULL, NULL}  |false |\n",
      "+---+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_students_v2 = [\n",
    "    (1, ('Jacob', None, 'Smith'), True),\n",
    "    (2, ('Linda', 'Mia', 'Jonh'), False),\n",
    "    (3, (None, None, None), False)\n",
    "]\n",
    "\n",
    "df_students_v2 = spark.createDataFrame(data=data_students_v2,schema=schema_students)\n",
    "df_students_v2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d2026-6775-4dbf-ba83-9e613258de46",
   "metadata": {},
   "source": [
    "## Struct Type Different Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08972d48-d16f-478b-8b98-9326d16012ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkValueError",
     "evalue": "[LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 2 and 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m data_students_v3 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     (\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJacob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmith\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m     (\u001b[38;5;241m2\u001b[39m, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinda\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJonh\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m     (\u001b[38;5;241m3\u001b[39m, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOliver\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJames\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJohnson\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThomas\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m df_students_v3 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data\u001b[38;5;241m=\u001b[39mdata_students_v3,schema\u001b[38;5;241m=\u001b[39mschema_students)\n\u001b[1;32m      8\u001b[0m df_students_v3\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/spark-3.5.2-bin-hadoop3/python/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/spark-3.5.2-bin-hadoop3/python/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/spark-3.5.2-bin-hadoop3/python/pyspark/sql/session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m~/spark-3.5.2-bin-hadoop3/python/pyspark/sql/session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m-> 1459\u001b[0m     verify_func(obj)\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/spark-3.5.2-bin-hadoop3/python/pyspark/sql/types.py:2201\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2201\u001b[0m         verify_value(obj)\n",
      "File \u001b[0;32m~/spark-3.5.2-bin-hadoop3/python/pyspark/sql/types.py:2174\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2165\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2166\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2171\u001b[0m             },\n\u001b[1;32m   2172\u001b[0m         )\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 2174\u001b[0m         verifier(v)\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2176\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m~/spark-3.5.2-bin-hadoop3/python/pyspark/sql/types.py:2201\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2201\u001b[0m         verify_value(obj)\n",
      "File \u001b[0;32m~/spark-3.5.2-bin-hadoop3/python/pyspark/sql/types.py:2164\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m   2163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(verifiers):\n\u001b[0;32m-> 2164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2165\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2166\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   2167\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2168\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfields\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2169\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg1_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(obj)),\n\u001b[1;32m   2170\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg2_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(verifiers)),\n\u001b[1;32m   2171\u001b[0m             },\n\u001b[1;32m   2172\u001b[0m         )\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[1;32m   2174\u001b[0m         verifier(v)\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 2 and 3."
     ]
    }
   ],
   "source": [
    "data_students_v3 = [\n",
    "    (1, ('Jacob', 'Smith'), True),\n",
    "    (2, ('Linda', 'Mia', 'Jonh'), False),\n",
    "    (3, ('Oliver', 'James', 'Johnson', 'Thomas'), False)\n",
    "]\n",
    "\n",
    "df_students_v3 = spark.createDataFrame(data=data_students_v3,schema=schema_students)\n",
    "df_students_v3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438bac48-3a37-4c3f-94ee-6c39d098e59e",
   "metadata": {},
   "source": [
    "## Map Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38fb6c1f-6667-4a82-b53c-b32a4afbaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_catalog = StructType([\n",
    "    StructField(\"id\",IntegerType(),False),\n",
    "    StructField(\"item\",StringType(),True),\n",
    "    StructField(\"description\",MapType(StringType(), StringType()),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d36238-3814-40b7-88b1-75c89395b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_catalog = [\n",
    "    (1, 'SmartTV', {\"brand\":\"LG\", \"status\":\"available\"}),\n",
    "    (2, 'Microwave', {\"category\":\"kitchen\", \"status\":None}),\n",
    "    (3, 'Smartphone', {\"brand\":\"Iphone\", \"model\":\"13Pro\", \"connectivity\": \"4G\", \"camera\": \"13px\"}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e2f351-b815-42f4-809d-8cf5f6688426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------------------------------------------------------------------+\n",
      "|id |item      |description                                                          |\n",
      "+---+----------+---------------------------------------------------------------------+\n",
      "|1  |SmartTV   |{brand -> LG, status -> available}                                   |\n",
      "|2  |Microwave |{category -> kitchen, status -> NULL}                                |\n",
      "|3  |Smartphone|{model -> 13Pro, connectivity -> 4G, camera -> 13px, brand -> Iphone}|\n",
      "+---+----------+---------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_catalog = spark.createDataFrame(data=data_catalog,schema=schema_catalog)\n",
    "df_catalog.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
