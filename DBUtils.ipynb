{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a04b08c-f6ec-465d-9d52-0248dc1d9f37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DBUTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "062d7268-e3f2-4428-988c-e588e7348762",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e750d763-db95-402c-ac0e-58ec4ddc042a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\n",
       "this package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\n",
       "another FileSystem URI.\n",
       "\n",
       "For more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n",
       "\n",
       "In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\n",
       "straightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\n",
       "translates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n",
       "    <h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2d59a1-30b8-4bc3-8bb3-f08d07e6790a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">/**<br /> * Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8.<br /> * <br /> * Example: head(\"/mnt/my-folder/my-file\")<br /> * <br /> * @param file FileSystem URI<br /> * @param maxBytes Maximum number of bytes to read<br /> * @return String containing contents of the file, possibly truncated to the max bytes.<br /> * @throws IllegalArgumentException if the file is actually a directory.<br /> */<br /><b>head(file: java.lang.String, maxBytes: int = 65536): java.lang.String</b></div><br />"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class = \"ansiout\">/**<br /> * Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8.<br /> * <br /> * Example: head(\"/mnt/my-folder/my-file\")<br /> * <br /> * @param file FileSystem URI<br /> * @param maxBytes Maximum number of bytes to read<br /> * @return String containing contents of the file, possibly truncated to the max bytes.<br /> * @throws IllegalArgumentException if the file is actually a directory.<br /> */<br /><b>head(file: java.lang.String, maxBytes: int = 65536): java.lang.String</b></div><br />",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.help(\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135fe8f4-aa5d-4068-83e9-0e9e3a7f223e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: 'id,name,guild,class,level,experience,health,is_alive,last_active,mentor_id\\r\\n1,Aragorn,Rangers,Warrior,25,9000,120.5,TRUE,2023-09-20 08:00:00 UTC,\\r\\n2,Legolas,Mirkwood,Archer,22,7500,100.75,TRUE,2023-09-21 09:00:00 UTC,\\r\\n3,Gimli,Erebor,Warrior,20,6000,130.25,TRUE,2023-09-22 07:00:00 UTC,\\r\\n4,Frodo,Shirefolk,Hobbit,12,2500,50.5,FALSE,2023-09-23 10:00:00 UTC,1\\r\\n5,Sam,Shirefolk,Hobbit,13,2750,52.5,TRUE,2023-09-23 10:05:00 UTC,1\\r\\n6,Gandalf,Istari,Mage,30,10000,90,TRUE,2023-09-20 11:00:00 UTC,\\r\\n7,Boromir,Gondor,Warrior,18,5500,115.5,FALSE,2023-09-21 12:00:00 UTC,\\r\\n8,Merry,Shirefolk,Hobbit,11,2200,48.5,TRUE,2023-09-22 10:00:00 UTC,4\\r\\n9,Pippin,Shirefolk,Hobbit,11,2100,47.5,TRUE,2023-09-22 10:01:00 UTC,4\\r\\n10,Sauron,Dark Lords,Mage,40,15000,85,FALSE,2023-09-23 01:00:00 UTC,\\r\\n11,Saruman,Istari,Mage,28,8500,88.5,FALSE,2023-09-20 03:00:00 UTC,6\\r\\n12,Elrond,Rivendell,Mage,27,8200,87.5,TRUE,2023-09-21 04:00:00 UTC,6\\r\\n13,Thranduil,Mirkwood,Archer,26,7900,97,TRUE,2023-09-22 02:00:00 UTC,2\\r\\n14,Eowyn,Rohan,Warrior,17,5200,112.5,TRUE,2023-09-23 05:00:00 UTC,7\\r\\n15,Faramir,Gondor,Archer,19,5700,118.5,TRUE,2023-09-20 06:00:00 UTC,7'"
     ]
    }
   ],
   "source": [
    "dbutils.fs.head('dbfs:/FileStore/tables/fantasy/characters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38d1c829-b49a-4517-9e76-397100e88ebd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a6dfbe-c4af-484c-89a0-3d2d9c39f61c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">\n",
       "The notebook module.\n",
       "  <h3></h3><b>exit(value: String): void</b> -> This method lets you exit a notebook with a value<br /><b>run(path: String, timeoutSeconds: int, arguments: Map): String</b> -> This method runs a notebook and returns its exit value<br /><br /></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class = \"ansiout\">\nThe notebook module.\n  <h3></h3><b>exit(value: String): void</b> -> This method lets you exit a notebook with a value<br /><b>run(path: String, timeoutSeconds: int, arguments: Map): String</b> -> This method runs a notebook and returns its exit value<br /><br /></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.notebook.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64fa02a4-71ba-421e-b603-fc695a264e31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2496984598370242>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnotebook\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/DeltaLake\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m60\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:205\u001B[0m, in \u001B[0;36mDBUtils.NotebookHandler.run\u001B[0;34m(self, path, timeout_seconds, arguments, _NotebookHandler__databricks_internal_cluster_spec)\u001B[0m\n",
       "\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, path, timeout_seconds, arguments\u001B[38;5;241m=\u001B[39m{}, __databricks_internal_cluster_spec\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n",
       "\u001B[1;32m    204\u001B[0m     arguments_scala_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoScalaMap(arguments)\n",
       "\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentry_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetDbutils\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnotebook\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout_seconds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marguments_scala_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m__databricks_internal_cluster_spec\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: To enable notebook workflows, please upgrade your Databricks subscription."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-2496984598370242>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnotebook\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/DeltaLake\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m60\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:205\u001B[0m, in \u001B[0;36mDBUtils.NotebookHandler.run\u001B[0;34m(self, path, timeout_seconds, arguments, _NotebookHandler__databricks_internal_cluster_spec)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, path, timeout_seconds, arguments\u001B[38;5;241m=\u001B[39m{}, __databricks_internal_cluster_spec\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    204\u001B[0m     arguments_scala_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoScalaMap(arguments)\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentry_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetDbutils\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnotebook\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout_seconds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marguments_scala_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m__databricks_internal_cluster_spec\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: To enable notebook workflows, please upgrade your Databricks subscription.",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: To enable notebook workflows, please upgrade your Databricks subscription.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.notebook.run(\"/DeltaLake\",60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf111ce4-c019-4a69-944f-a67547df6f71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88bc7425-700d-48eb-b71e-52346be51fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\"><b>dbutils.widgets</b> provides utilities for working with notebook widgets. You can create\n",
       "different types of widgets and get their bound value.\n",
       "\n",
       "For more info about a method, use <b>dbutils.widgets.help(\"methodName\")</b>.\n",
       "    <h3></h3><b>combobox(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a combobox input widget with a given name, default value and choices<br /><b>dropdown(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a dropdown input widget a with given name, default value and choices<br /><b>get(name: String): String</b> -> Retrieves current value of an input widget<br /><b>getArgument(name: String, optional: String): String</b> -> (DEPRECATED) Equivalent to get<br /><b>multiselect(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a multiselect input widget with a given name, default value and choices<br /><b>remove(name: String): void</b> -> Removes an input widget from the notebook<br /><b>removeAll: void</b> -> Removes all widgets in the notebook<br /><b>text(name: String, defaultValue: String, label: String): void</b> -> Creates a text input widget with a given name and default value<br /><br /></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class = \"ansiout\"><b>dbutils.widgets</b> provides utilities for working with notebook widgets. You can create\ndifferent types of widgets and get their bound value.\n\nFor more info about a method, use <b>dbutils.widgets.help(\"methodName\")</b>.\n    <h3></h3><b>combobox(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a combobox input widget with a given name, default value and choices<br /><b>dropdown(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a dropdown input widget a with given name, default value and choices<br /><b>get(name: String): String</b> -> Retrieves current value of an input widget<br /><b>getArgument(name: String, optional: String): String</b> -> (DEPRECATED) Equivalent to get<br /><b>multiselect(name: String, defaultValue: String, choices: Seq, label: String): void</b> -> Creates a multiselect input widget with a given name, default value and choices<br /><b>remove(name: String): void</b> -> Removes an input widget from the notebook<br /><b>removeAll: void</b> -> Removes all widgets in the notebook<br /><b>text(name: String, defaultValue: String, label: String): void</b> -> Creates a text input widget with a given name and default value<br /><br /></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b603a9b8-4448-4f80-b39a-3f57ed4ce202",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"App\",\"\")\n",
    "dbutils.widgets.text(\"Enviroment\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c46f8f-b739-4c01-8c36-f8b221105514",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "App name: Spark\nEnviroment name: Dev\n"
     ]
    }
   ],
   "source": [
    "var_app = dbutils.widgets.get(\"App\")\n",
    "var_enviroment = dbutils.widgets.get(\"Enviroment\")\n",
    "\n",
    "print(f\"App name: {var_app}\")\n",
    "print(f\"Enviroment name: {var_enviroment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb6c401-070f-4a53-b073-a896ce932cbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "520fc0fb-dcb1-4ee8-b144-d0b3af3db0f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cabda7e6-5455-4541-976c-f7a46aaf4ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">\n",
       "Provides utilities for leveraging secrets within notebooks.\n",
       "Databricks documentation for more info.\n",
       "    <h3></h3><b>get(scope: String, key: String): String</b> -> Gets the string representation of a secret value with scope and key<br /><b>getBytes(scope: String, key: String): byte[]</b> -> Gets the bytes representation of a secret value with scope and key<br /><b>list(scope: String): Seq</b> -> Lists secret metadata for secrets within a scope<br /><b>listScopes: Seq</b> -> Lists secret scopes<br /><br /></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class = \"ansiout\">\nProvides utilities for leveraging secrets within notebooks.\nDatabricks documentation for more info.\n    <h3></h3><b>get(scope: String, key: String): String</b> -> Gets the string representation of a secret value with scope and key<br /><b>getBytes(scope: String, key: String): byte[]</b> -> Gets the bytes representation of a secret value with scope and key<br /><b>list(scope: String): Seq</b> -> Lists secret metadata for secrets within a scope<br /><b>listScopes: Seq</b> -> Lists secret scopes<br /><br /></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.secrets.help()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DBUtils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
