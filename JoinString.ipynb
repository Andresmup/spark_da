{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62871557-a63c-4da8-9769-70d8c792c4a9",
   "metadata": {},
   "source": [
    "# JOIN STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d320ebc2-1e42-455d-9980-ccf45d7a8a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1330871b-7843-4401-a3cc-ae8c94ab25ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/19 18:11:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa79eeb-4c87-4cfe-b141-da0ecee1003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_data = [\n",
    "    (1, \"Charlie Wood\", \"2023-01-20\", True, 250000),\n",
    "    (2, \"Alice Johnson\", \"2021-03-15\", True, 180000),\n",
    "    (3, \"Bob Smith\", \"2019-06-12\", False, 220000),\n",
    "    (4, \"Diane Brown\", \"2022-11-05\", True, 200000),\n",
    "    (5, \"Eve Davis\", \"2020-08-30\", False, 195000),\n",
    "    (6, \"Frank Martin\", \"2018-12-22\", True, 240000),\n",
    "    (7, \"Grace Lee\", \"2023-02-10\", True, 210000),\n",
    "    (8, \"Henry Clark\", \"2022-05-17\", True, 230000),\n",
    "    (9, \"Irene Turner\", \"2017-09-25\", False, 175000),\n",
    "    (10, \"Jack White\", \"2021-07-13\", True, 190000)\n",
    "]\n",
    "\n",
    "employee_schema = [\"id\",\"name\",\"date_of_join\",\"active\",\"anual_salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27597c79-0886-4049-aa88-77e83d44dcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+------+------------+\n",
      "| id|         name|date_of_join|active|anual_salary|\n",
      "+---+-------------+------------+------+------------+\n",
      "|  1| Charlie Wood|  2023-01-20|  true|      250000|\n",
      "|  2|Alice Johnson|  2021-03-15|  true|      180000|\n",
      "|  3|    Bob Smith|  2019-06-12| false|      220000|\n",
      "|  4|  Diane Brown|  2022-11-05|  true|      200000|\n",
      "|  5|    Eve Davis|  2020-08-30| false|      195000|\n",
      "|  6| Frank Martin|  2018-12-22|  true|      240000|\n",
      "|  7|    Grace Lee|  2023-02-10|  true|      210000|\n",
      "|  8|  Henry Clark|  2022-05-17|  true|      230000|\n",
      "|  9| Irene Turner|  2017-09-25| false|      175000|\n",
      "| 10|   Jack White|  2021-07-13|  true|      190000|\n",
      "+---+-------------+------------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee = spark.createDataFrame(employee_data,employee_schema)\n",
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1248669e-c654-4a75-b4ce-2914d4174b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- date_of_join: string (nullable = true)\n",
      " |-- active: boolean (nullable = true)\n",
      " |-- anual_salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "363a7bb2-1ea0-49fa-bf0e-6eb279b6ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'date_of_join', 'active', 'anual_salary']\n"
     ]
    }
   ],
   "source": [
    "list_columns = df_employee.columns\n",
    "\n",
    "print(list_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab5246f-2d2d-4af4-8bf2-ba197ea252a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name,date_of_join,active,anual_salary\n"
     ]
    }
   ],
   "source": [
    "list_joined_comma = \",\".join(list_columns)\n",
    "\n",
    "print(list_joined_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "058f877e-2e5c-4bd2-8c5a-3e1c3e807db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idnamedate_of_joinactiveanual_salary\n"
     ]
    }
   ],
   "source": [
    "list_joined_blank = \"\".join(list_columns)\n",
    "\n",
    "print(list_joined_blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd46ab2-fb0f-4e4f-9adc-9b63d5dca9e2",
   "metadata": {},
   "source": [
    "## Use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074005df-4cb1-4ce4-8ed9-a5fec0201090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'date_of_join', 'active', 'anual_salary']\n"
     ]
    }
   ],
   "source": [
    "print(list_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9016e03c-88ba-493c-b2cc-93e3f4cbd2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target.id = Source.id AND Target.name = Source.name AND Target.date_of_join = Source.date_of_join AND Target.active = Source.active AND Target.anual_salary = Source.anual_salary\n"
     ]
    }
   ],
   "source": [
    "join_string = \" AND \".join(list(map(lambda x: (\"Target.\" + x + \" = Source.\" + x),list_columns)))\n",
    "\n",
    "print(join_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e1b52df-17c8-41bc-b486-31d349d2ed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target.id = Source.id,Target.name = Source.name,Target.date_of_join = Source.date_of_join,Target.active = Source.active,Target.anual_salary = Source.anual_salary\n"
     ]
    }
   ],
   "source": [
    "update_string = \",\".join(list(map(lambda x: (\"Target.\" + x + \" = Source.\" + x),list_columns)))\n",
    "\n",
    "print(update_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8fe6fde-f0c9-4983-a14e-fa69908b20aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source.id,Source.name,Source.date_of_join,Source.active,Source.anual_salary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 33446)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andresmunozpampillon/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/andresmunozpampillon/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/andresmunozpampillon/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/andresmunozpampillon/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/andresmunozpampillon/spark-3.5.2-bin-hadoop3/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/andresmunozpampillon/spark-3.5.2-bin-hadoop3/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/home/andresmunozpampillon/spark-3.5.2-bin-hadoop3/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/andresmunozpampillon/spark-3.5.2-bin-hadoop3/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "source_insert_string = \",\".join(list(map(lambda x: (\"Source.\" + x),list_columns)))\n",
    "\n",
    "print(source_insert_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f73c8-0e6b-440c-9388-399bdad57783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
