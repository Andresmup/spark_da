{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfef8824-ac39-4f95-8343-111ad9f90fba",
   "metadata": {},
   "source": [
    "# EXPLAIN PLAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac21d86-7d31-4b66-a926-8e67be3c2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa4bf0e-a90f-45ba-a607-828fa697cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/27 23:46:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85582ff3-71dd-4088-a60d-3138080ca134",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_employee = [(1, \"Carl Mike\", \"m\", 170000, 1),\n",
    "                 (2, \"Mikel Clark\", \"m\", 254300, 2),\n",
    "                 (3, \"Bob Smith\", \"m\", 220000, 3),\n",
    "                 (4, \"Mary Scala\", \"f\", 230000, 1),\n",
    "                 (5, \"Susan Liam\", \"f\", 150000, 1),\n",
    "                 (6, \"Xi Wuan\", \"f\", 150000, 2),\n",
    "                 (7, \"Kyla Stewart\", \"f\", 185000, 2),\n",
    "                 (8, \"Mia Lebrin\", \"f\", 242000, 1)]\n",
    "\n",
    "\n",
    "schema_employee = \"id INTEGER, name STRING, gender STRING, salary INTEGER, dept_id INTEGER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a6f9da-acd7-4117-b9ac-beabcc428ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+------+-------+\n",
      "| id|        name|gender|salary|dept_id|\n",
      "+---+------------+------+------+-------+\n",
      "|  1|   Carl Mike|     m|170000|      1|\n",
      "|  2| Mikel Clark|     m|254300|      2|\n",
      "|  3|   Bob Smith|     m|220000|      3|\n",
      "|  4|  Mary Scala|     f|230000|      1|\n",
      "|  5|  Susan Liam|     f|150000|      1|\n",
      "|  6|     Xi Wuan|     f|150000|      2|\n",
      "|  7|Kyla Stewart|     f|185000|      2|\n",
      "|  8|  Mia Lebrin|     f|242000|      1|\n",
      "+---+------------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee = spark.createDataFrame(data=data_employee, schema=schema_employee)\n",
    "\n",
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d4e9751-56a9-4bf2-820a-83996be9e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_department = [(1, \"IT\", 1),\n",
    "                 (2, \"Sales\", 2),\n",
    "                 (3, \"HR\", 2)]\n",
    "\n",
    "\n",
    "schema_department = \"id INTEGER, dept STRING, floor INTEGER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb91fb2e-bfd5-4644-9d99-87606d9dc720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id| dept|floor|\n",
      "+---+-----+-----+\n",
      "|  1|   IT|    1|\n",
      "|  2|Sales|    2|\n",
      "|  3|   HR|    2|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_department = spark.createDataFrame(data=data_department, schema=schema_department)\n",
    "\n",
    "df_department.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d70a63-fee8-44a2-b4d3-d3a07ff38e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4021b1dd-31b1-4f0f-8f3b-d773ef176c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n",
      "| dept|round(avg(salary), 2)|\n",
      "+-----+---------------------+\n",
      "|Sales|            196433.33|\n",
      "|   HR|             220000.0|\n",
      "|   IT|             198000.0|\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join = df_employee.join(df_department, df_employee.dept_id == df_department.id, \"inner\") \\\n",
    "    .withColumn(\"bonus\",col(\"salary\")*0.1) \\\n",
    "    .groupBy(\"dept\").agg(round(avg(\"salary\"),2)).alias(\"average_salary\")\n",
    "\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104e786-47e6-4cb4-bd4f-4014081f0acb",
   "metadata": {},
   "source": [
    "## Explain Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04cb26d9-4d80-48f9-99f6-3e429fbe83fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept#32], functions=[avg(salary#3)])\n",
      "   +- Exchange hashpartitioning(dept#32, 200), ENSURE_REQUIREMENTS, [plan_id=287]\n",
      "      +- HashAggregate(keys=[dept#32], functions=[partial_avg(salary#3)])\n",
      "         +- Project [salary#3, dept#32]\n",
      "            +- SortMergeJoin [dept_id#4], [id#31], Inner\n",
      "               :- Sort [dept_id#4 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(dept_id#4, 200), ENSURE_REQUIREMENTS, [plan_id=279]\n",
      "               :     +- Project [salary#3, dept_id#4]\n",
      "               :        +- Filter isnotnull(dept_id#4)\n",
      "               :           +- Scan ExistingRDD[id#0,name#1,gender#2,salary#3,dept_id#4]\n",
      "               +- Sort [id#31 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#31, 200), ENSURE_REQUIREMENTS, [plan_id=280]\n",
      "                     +- Project [id#31, dept#32]\n",
      "                        +- Filter isnotnull(id#31)\n",
      "                           +- Scan ExistingRDD[id#31,dept#32,floor#33]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed213e5d-73f9-4c07-ba65-1a0dd930fb2e",
   "metadata": {},
   "source": [
    "## Plan Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08188ec-5cab-46e6-bb52-6f16f4778882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "SubqueryAlias average_salary\n",
      "+- Aggregate [dept#32], [dept#32, round(avg(salary#3), 2) AS round(avg(salary), 2)#86]\n",
      "   +- Project [id#0, name#1, gender#2, salary#3, dept_id#4, id#31, dept#32, floor#33, (cast(salary#3 as double) * 0.1) AS bonus#66]\n",
      "      +- Join Inner, (dept_id#4 = id#31)\n",
      "         :- LogicalRDD [id#0, name#1, gender#2, salary#3, dept_id#4], false\n",
      "         +- LogicalRDD [id#31, dept#32, floor#33], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "dept: string, round(avg(salary), 2): double\n",
      "SubqueryAlias average_salary\n",
      "+- Aggregate [dept#32], [dept#32, round(avg(salary#3), 2) AS round(avg(salary), 2)#86]\n",
      "   +- Project [id#0, name#1, gender#2, salary#3, dept_id#4, id#31, dept#32, floor#33, (cast(salary#3 as double) * 0.1) AS bonus#66]\n",
      "      +- Join Inner, (dept_id#4 = id#31)\n",
      "         :- LogicalRDD [id#0, name#1, gender#2, salary#3, dept_id#4], false\n",
      "         +- LogicalRDD [id#31, dept#32, floor#33], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [dept#32], [dept#32, round(avg(salary#3), 2) AS round(avg(salary), 2)#86]\n",
      "+- Project [salary#3, dept#32]\n",
      "   +- Join Inner, (dept_id#4 = id#31)\n",
      "      :- Project [salary#3, dept_id#4]\n",
      "      :  +- Filter isnotnull(dept_id#4)\n",
      "      :     +- LogicalRDD [id#0, name#1, gender#2, salary#3, dept_id#4], false\n",
      "      +- Project [id#31, dept#32]\n",
      "         +- Filter isnotnull(id#31)\n",
      "            +- LogicalRDD [id#31, dept#32, floor#33], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept#32], functions=[avg(salary#3)], output=[dept#32, round(avg(salary), 2)#86])\n",
      "   +- Exchange hashpartitioning(dept#32, 200), ENSURE_REQUIREMENTS, [plan_id=287]\n",
      "      +- HashAggregate(keys=[dept#32], functions=[partial_avg(salary#3)], output=[dept#32, sum#97, count#98L])\n",
      "         +- Project [salary#3, dept#32]\n",
      "            +- SortMergeJoin [dept_id#4], [id#31], Inner\n",
      "               :- Sort [dept_id#4 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(dept_id#4, 200), ENSURE_REQUIREMENTS, [plan_id=279]\n",
      "               :     +- Project [salary#3, dept_id#4]\n",
      "               :        +- Filter isnotnull(dept_id#4)\n",
      "               :           +- Scan ExistingRDD[id#0,name#1,gender#2,salary#3,dept_id#4]\n",
      "               +- Sort [id#31 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#31, 200), ENSURE_REQUIREMENTS, [plan_id=280]\n",
      "                     +- Project [id#31, dept#32]\n",
      "                        +- Filter isnotnull(id#31)\n",
      "                           +- Scan ExistingRDD[id#31,dept#32,floor#33]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540ac82-21ac-414e-a9a1-7829654eed24",
   "metadata": {},
   "source": [
    "## Simple Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b729c25-3be5-4416-9e1a-d03ea3305356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept#32], functions=[avg(salary#3)])\n",
      "   +- Exchange hashpartitioning(dept#32, 200), ENSURE_REQUIREMENTS, [plan_id=287]\n",
      "      +- HashAggregate(keys=[dept#32], functions=[partial_avg(salary#3)])\n",
      "         +- Project [salary#3, dept#32]\n",
      "            +- SortMergeJoin [dept_id#4], [id#31], Inner\n",
      "               :- Sort [dept_id#4 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(dept_id#4, 200), ENSURE_REQUIREMENTS, [plan_id=279]\n",
      "               :     +- Project [salary#3, dept_id#4]\n",
      "               :        +- Filter isnotnull(dept_id#4)\n",
      "               :           +- Scan ExistingRDD[id#0,name#1,gender#2,salary#3,dept_id#4]\n",
      "               +- Sort [id#31 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#31, 200), ENSURE_REQUIREMENTS, [plan_id=280]\n",
      "                     +- Project [id#31, dept#32]\n",
      "                        +- Filter isnotnull(id#31)\n",
      "                           +- Scan ExistingRDD[id#31,dept#32,floor#33]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7c970-3447-4bd4-aa55-133f53143170",
   "metadata": {},
   "source": [
    "## Simple Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0b6f06-3eb0-4f5c-8763-4794f4086346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "SubqueryAlias average_salary\n",
      "+- Aggregate [dept#32], [dept#32, round(avg(salary#3), 2) AS round(avg(salary), 2)#86]\n",
      "   +- Project [id#0, name#1, gender#2, salary#3, dept_id#4, id#31, dept#32, floor#33, (cast(salary#3 as double) * 0.1) AS bonus#66]\n",
      "      +- Join Inner, (dept_id#4 = id#31)\n",
      "         :- LogicalRDD [id#0, name#1, gender#2, salary#3, dept_id#4], false\n",
      "         +- LogicalRDD [id#31, dept#32, floor#33], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "dept: string, round(avg(salary), 2): double\n",
      "SubqueryAlias average_salary\n",
      "+- Aggregate [dept#32], [dept#32, round(avg(salary#3), 2) AS round(avg(salary), 2)#86]\n",
      "   +- Project [id#0, name#1, gender#2, salary#3, dept_id#4, id#31, dept#32, floor#33, (cast(salary#3 as double) * 0.1) AS bonus#66]\n",
      "      +- Join Inner, (dept_id#4 = id#31)\n",
      "         :- LogicalRDD [id#0, name#1, gender#2, salary#3, dept_id#4], false\n",
      "         +- LogicalRDD [id#31, dept#32, floor#33], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [dept#32], [dept#32, round(avg(salary#3), 2) AS round(avg(salary), 2)#86]\n",
      "+- Project [salary#3, dept#32]\n",
      "   +- Join Inner, (dept_id#4 = id#31)\n",
      "      :- Project [salary#3, dept_id#4]\n",
      "      :  +- Filter isnotnull(dept_id#4)\n",
      "      :     +- LogicalRDD [id#0, name#1, gender#2, salary#3, dept_id#4], false\n",
      "      +- Project [id#31, dept#32]\n",
      "         +- Filter isnotnull(id#31)\n",
      "            +- LogicalRDD [id#31, dept#32, floor#33], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept#32], functions=[avg(salary#3)], output=[dept#32, round(avg(salary), 2)#86])\n",
      "   +- Exchange hashpartitioning(dept#32, 200), ENSURE_REQUIREMENTS, [plan_id=287]\n",
      "      +- HashAggregate(keys=[dept#32], functions=[partial_avg(salary#3)], output=[dept#32, sum#97, count#98L])\n",
      "         +- Project [salary#3, dept#32]\n",
      "            +- SortMergeJoin [dept_id#4], [id#31], Inner\n",
      "               :- Sort [dept_id#4 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(dept_id#4, 200), ENSURE_REQUIREMENTS, [plan_id=279]\n",
      "               :     +- Project [salary#3, dept_id#4]\n",
      "               :        +- Filter isnotnull(dept_id#4)\n",
      "               :           +- Scan ExistingRDD[id#0,name#1,gender#2,salary#3,dept_id#4]\n",
      "               +- Sort [id#31 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#31, 200), ENSURE_REQUIREMENTS, [plan_id=280]\n",
      "                     +- Project [id#31, dept#32]\n",
      "                        +- Filter isnotnull(id#31)\n",
      "                           +- Scan ExistingRDD[id#31,dept#32,floor#33]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038141b1-00ce-4ac7-bb1c-cfc9600c18e2",
   "metadata": {},
   "source": [
    "## Formatted Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7082b840-b896-4b58-812e-b195f90f10ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (16)\n",
      "+- HashAggregate (15)\n",
      "   +- Exchange (14)\n",
      "      +- HashAggregate (13)\n",
      "         +- Project (12)\n",
      "            +- SortMergeJoin Inner (11)\n",
      "               :- Sort (5)\n",
      "               :  +- Exchange (4)\n",
      "               :     +- Project (3)\n",
      "               :        +- Filter (2)\n",
      "               :           +- Scan ExistingRDD (1)\n",
      "               +- Sort (10)\n",
      "                  +- Exchange (9)\n",
      "                     +- Project (8)\n",
      "                        +- Filter (7)\n",
      "                           +- Scan ExistingRDD (6)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD\n",
      "Output [5]: [id#0, name#1, gender#2, salary#3, dept_id#4]\n",
      "Arguments: [id#0, name#1, gender#2, salary#3, dept_id#4], MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter\n",
      "Input [5]: [id#0, name#1, gender#2, salary#3, dept_id#4]\n",
      "Condition : isnotnull(dept_id#4)\n",
      "\n",
      "(3) Project\n",
      "Output [2]: [salary#3, dept_id#4]\n",
      "Input [5]: [id#0, name#1, gender#2, salary#3, dept_id#4]\n",
      "\n",
      "(4) Exchange\n",
      "Input [2]: [salary#3, dept_id#4]\n",
      "Arguments: hashpartitioning(dept_id#4, 200), ENSURE_REQUIREMENTS, [plan_id=279]\n",
      "\n",
      "(5) Sort\n",
      "Input [2]: [salary#3, dept_id#4]\n",
      "Arguments: [dept_id#4 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(6) Scan ExistingRDD\n",
      "Output [3]: [id#31, dept#32, floor#33]\n",
      "Arguments: [id#31, dept#32, floor#33], MapPartitionsRDD[11] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(7) Filter\n",
      "Input [3]: [id#31, dept#32, floor#33]\n",
      "Condition : isnotnull(id#31)\n",
      "\n",
      "(8) Project\n",
      "Output [2]: [id#31, dept#32]\n",
      "Input [3]: [id#31, dept#32, floor#33]\n",
      "\n",
      "(9) Exchange\n",
      "Input [2]: [id#31, dept#32]\n",
      "Arguments: hashpartitioning(id#31, 200), ENSURE_REQUIREMENTS, [plan_id=280]\n",
      "\n",
      "(10) Sort\n",
      "Input [2]: [id#31, dept#32]\n",
      "Arguments: [id#31 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(11) SortMergeJoin\n",
      "Left keys [1]: [dept_id#4]\n",
      "Right keys [1]: [id#31]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [2]: [salary#3, dept#32]\n",
      "Input [4]: [salary#3, dept_id#4, id#31, dept#32]\n",
      "\n",
      "(13) HashAggregate\n",
      "Input [2]: [salary#3, dept#32]\n",
      "Keys [1]: [dept#32]\n",
      "Functions [1]: [partial_avg(salary#3)]\n",
      "Aggregate Attributes [2]: [sum#95, count#96L]\n",
      "Results [3]: [dept#32, sum#97, count#98L]\n",
      "\n",
      "(14) Exchange\n",
      "Input [3]: [dept#32, sum#97, count#98L]\n",
      "Arguments: hashpartitioning(dept#32, 200), ENSURE_REQUIREMENTS, [plan_id=287]\n",
      "\n",
      "(15) HashAggregate\n",
      "Input [3]: [dept#32, sum#97, count#98L]\n",
      "Keys [1]: [dept#32]\n",
      "Functions [1]: [avg(salary#3)]\n",
      "Aggregate Attributes [1]: [avg(salary#3)#85]\n",
      "Results [2]: [dept#32, round(avg(salary#3)#85, 2) AS round(avg(salary), 2)#86]\n",
      "\n",
      "(16) AdaptiveSparkPlan\n",
      "Output [2]: [dept#32, round(avg(salary), 2)#86]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1a5ea-7351-450d-b678-b2e7b226566f",
   "metadata": {},
   "source": [
    "## Cost Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba321c19-1d92-4a86-adf4-0ae2db1a5c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Aggregate [dept#32], [dept#32, round(avg(salary#3), 2) AS round(avg(salary), 2)#86], Statistics(sizeInBytes=1.81E+37 B)\n",
      "+- Project [salary#3, dept#32], Statistics(sizeInBytes=1.61E+37 B)\n",
      "   +- Join Inner, (dept_id#4 = id#31), Statistics(sizeInBytes=2.02E+37 B)\n",
      "      :- Project [salary#3, dept_id#4], Statistics(sizeInBytes=2.1 EiB)\n",
      "      :  +- Filter isnotnull(dept_id#4), Statistics(sizeInBytes=8.0 EiB)\n",
      "      :     +- LogicalRDD [id#0, name#1, gender#2, salary#3, dept_id#4], false, Statistics(sizeInBytes=8.0 EiB)\n",
      "      +- Project [id#31, dept#32], Statistics(sizeInBytes=7.1 EiB)\n",
      "         +- Filter isnotnull(id#31), Statistics(sizeInBytes=8.0 EiB)\n",
      "            +- LogicalRDD [id#31, dept#32, floor#33], false, Statistics(sizeInBytes=8.0 EiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept#32], functions=[avg(salary#3)], output=[dept#32, round(avg(salary), 2)#86])\n",
      "   +- Exchange hashpartitioning(dept#32, 200), ENSURE_REQUIREMENTS, [plan_id=287]\n",
      "      +- HashAggregate(keys=[dept#32], functions=[partial_avg(salary#3)], output=[dept#32, sum#97, count#98L])\n",
      "         +- Project [salary#3, dept#32]\n",
      "            +- SortMergeJoin [dept_id#4], [id#31], Inner\n",
      "               :- Sort [dept_id#4 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(dept_id#4, 200), ENSURE_REQUIREMENTS, [plan_id=279]\n",
      "               :     +- Project [salary#3, dept_id#4]\n",
      "               :        +- Filter isnotnull(dept_id#4)\n",
      "               :           +- Scan ExistingRDD[id#0,name#1,gender#2,salary#3,dept_id#4]\n",
      "               +- Sort [id#31 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#31, 200), ENSURE_REQUIREMENTS, [plan_id=280]\n",
      "                     +- Project [id#31, dept#32]\n",
      "                        +- Filter isnotnull(id#31)\n",
      "                           +- Scan ExistingRDD[id#31,dept#32,floor#33]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 60606)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andresmunozpampillon/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/andresmunozpampillon/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/andresmunozpampillon/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/andresmunozpampillon/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/andresmunozpampillon/spark-3.5.2-bin-hadoop3/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/andresmunozpampillon/spark-3.5.2-bin-hadoop3/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/home/andresmunozpampillon/spark-3.5.2-bin-hadoop3/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/andresmunozpampillon/spark-3.5.2-bin-hadoop3/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_join.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087f72b-5513-4462-9545-1c47837412b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
